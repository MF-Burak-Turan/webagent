import asyncio
from src.scraper import scrape_website
from src.agent import WebAgent

async def main():
    url = input("\nURL: ")
    query = input("Bu sayfada neyi bulmak/takip etmek istiyorsunuz? \n(Ã–rn: 'FiyatÄ± 20.000 TL altÄ±ndaki laptoplar'): ")
    
    if not url.startswith("http"):
        print("âŒ Hata: GeÃ§erli bir URL girmelisiniz!")
        return

    print(f"\n[1/2] ğŸ” Sayfa iÃ§eriÄŸi Ã§ekiliyor: {url}...")
    
    try:
        crawl_result = await scrape_website(url)
        markdown_content = crawl_result.markdown if hasattr(crawl_result, 'markdown') else crawl_result

        print("[2/2] ğŸ¤– Yapay zeka verileri analiz ediyor...")
        
        agent = WebAgent()
        result = agent.process_content(markdown_content, query)
        
        print("\n" + "="*50)
        print(f"ğŸ“ Ã–ZET: {result.summary}") 
        print("="*50)

        if not result.items:
            print("Sorgunuza uygun Ã¼rÃ¼n bulunamadÄ±.")
        else:
            for i, item in enumerate(result.items, 1):
                print(f"\n{i}. {item.title.upper()}")
                print(f"   ğŸ“Š Durum/DeÄŸer: {item.value}")
                
                if item.description and item.description != "Bilgi yok":
                    print(f"   â„¹ï¸  AÃ§Ä±klama: {item.description}")
                
                if item.tags:
                    print(f"   ğŸ·ï¸  Etiketler: {', '.join(item.tags)}")

        print("\n" + "="*50)

    except Exception as e:
        print(f"\nâŒ Bir hata oluÅŸtu: {e}")

if __name__ == "__main__":
    asyncio.run(main())